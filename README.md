# WGS Data Analysis Pipeline: From FASTQ to VCF

## ðŸ“‹ Overview

This manual describes the Standard Operating Procedure (SOP) for processing Trio Whole Genome Sequencing (WGS) data (Father, Mother, Proband). The pipeline operates in a High-Performance Computing (HPC) Linux environment.

**Goal:** Transform raw sequencing data (`.fastq`) into annotated variant calls (`.vcf`) for clinical interpretation.
**Reference Genome:** GRCh38 (hg38)

## ðŸ›  Prerequisites & Environment

This pipeline utilizes `spack` for package management. Ensure the following tools are installed/loadable:

* **System Tools:** `screen`, `time` 
* **Bioinformatics Tools:** 
	* `fastp` (QC & Trimming) 
	* `bwa` (Alignment) 
	* `samtools` (BAM processing) 
	* `gatk` (Variant Calling - v4.x) 
	* `bcftools` (VCF normalization)

## ðŸ“‚ Directory Structure Recommendation

To ensure the commands below work smoothly, organize your project directory as follows:

``` 
/path/to/project/ 
â”œâ”€â”€ ref/ # Reference genome files (fasta, dbsnp, etc.) 
â”œâ”€â”€ <FAMILY_ID>/ # e.g., family_123 
	â”œâ”€â”€ fastq/ # Raw and merged FASTQ files 
	â”œâ”€â”€ bam/ # Aligned BAM files 
	â””â”€â”€ vcf/ # Final VCF files
```

### Reference Directory (`/ref`)

The reference genome folder must contain the **GRCh38 FASTA file** and all associated **indices** (generated by BWA, Samtools, and GATK). It should also include known variant sites for BQSR.

**Required Files in `ref/`:**
- **Reference Genome:**
	- `Homo_sapiens_assembly38_plus.fasta` (Main FASTA)
	- `Homo_sapiens_assembly38_plus.fasta.fai` (Samtools Index)
	- `Homo_sapiens_assembly38_plus.dict` (GATK Dictionary)
- **BWA Indices:**
	- `Homo_sapiens_assembly38_plus.fasta.amb`
	- `Homo_sapiens_assembly38_plus.fasta.ann`
	- `Homo_sapiens_assembly38_plus.fasta.bwt`
	- `Homo_sapiens_assembly38_plus.fasta.pac`
	- `Homo_sapiens_assembly38_plus.fasta.sa`
- **Known Sites (for BQSR):**
	- `dbsnp.vcf.gz` & `dbsnp.vcf.gz.tbi`
	- `Mills_indels.vcf.gz` & `Mills_indels.vcf.gz.tbi`

### Input Data Directory (`/fastq`)

Place the raw FASTQ files from the sequencer here.

**Naming Convention:** The script assumes files follow the Illumina pattern:
- `<SampleID-1>_L001_R1.fastq.gz`
- `<SampleID-1>_L001_R2.fastq.gz`
- `<SampleID-1>_L002_R1.fastq.gz`
- `<SampleID-1>_L002_R2.fastq.gz`
- `<SampleID-2>_L001_R1.fastq.gz`
- `<SampleID-2>_L001_R2.fastq.gz`
- `<SampleID-2>_L002_R1.fastq.gz`
- `<SampleID-2>_L002_R2.fastq.gz`
- `<SampleID-3>_L001_R1.fastq.gz`
- `<SampleID-3>_L001_R2.fastq.gz`
- `<SampleID-3>_L002_R1.fastq.gz`
- `<SampleID-3>_L002_R2.fastq.gz`

## ðŸš€ Step-by-Step Pipeline

### 0. Session Management (Screen)

**Why?** WGS analysis takes a long time. Using `screen` prevents the process from stopping if your SSH connection drops.

```
# 1. Create a new session
screen -S pipeline_session

# 2. To detach (leave the session running in background): 
# Press "Ctrl + A", then press "D"

# 3. To resume (re-connect):
screen -r pipeline_session

# 4. To kill the session (when finished):
exit
# OR
screen -X -S pipeline_session quit
```

### 1. Data Preparation (FASTQ)

**Location:** `/<project_path>/<family_id>/fastq`

#### 1-1. Merge Split FASTQ Files

Illumina sequencers often output data split into multiple lanes (L001, L002). We must merge them into a single file per read (R1, R2).

- **Input:** `*_L001_R1.fastq.gz`, `*_L002_R1.fastq.gz` ...
- **Output:** `SampleID_R1.fastq.gz`

Define Sample IDs (Change these for your specific family)
Example: 101 (Mother), 102 (Father), 103 (Proband)

```
SAMPLES="101 102 103"

for sid in $SAMPLES
do
  echo "Processing sample ${sid}..."
  
  # Merge R1
  cat RS-*-${sid}-*L001_R1_001.fastq.gz RS-*-${sid}-*L002_R1_001.fastq.gz > ${sid}_R1.fastq.gz
  
  # Merge R2
  cat RS-*-${sid}-*L001_R2_001.fastq.gz RS-*-${sid}-*L002_R2_001.fastq.gz > ${sid}_R2.fastq.gz
  
  echo "Finished merging sample ${sid}"
done
```

#### 1-2. Quality Control & Trimming

**Why?** Remove low-quality bases and adapters to improve alignment accuracy. 

```
spack load fastp

for sid in $SAMPLES
do
  echo "ðŸ”§ Trimming sample ${sid}..."

  fastp \
    -i ${sid}_R1.fastq.gz \
    -I ${sid}_R2.fastq.gz \
    -o ${sid}_trimmed_R1.fastq.gz \
    -O ${sid}_trimmed_R2.fastq.gz \
    -h ${sid}_fastp.html \
    -j ${sid}_fastp.json \
    -w 16

  echo "Finished trimming ${sid}"
done
```

### 2. Alignment (FASTQ â†’ SAM)

**Location:** `/<project_path>/<family_id>/bam`

**Why?** Map the short DNA reads to the human reference genome (GRCh38). 

```
spack load bwa

# Set Paths
FASTQ_DIR=../fastq
REF_DIR=/path/to/project/ref
REF=${REF_DIR}/Homo_sapiens_assembly38_plus.fasta

for sid in $SAMPLES
do
  echo "Aligning sample ${sid}..."

  bwa mem -t 32 \
    -R "@RG\tID:${sid}\tPL:ILLUMINA\tSM:${sid}" \
    ${REF} \
    ${FASTQ_DIR}/${sid}_trimmed_R1.fastq.gz \
    ${FASTQ_DIR}/${sid}_trimmed_R2.fastq.gz \
    > ${sid}.sam

  echo "Finished ${sid}.sam"
done
```

### 3. Post-Processing (SAM â†’ BAM)

**Location:** `/<project_path>/<family_id>/bam`

This step involves multiple sub-steps to prepare the file for variant calling:
1. **Compress & Sort:** Convert large SAM (text) to BAM (binary) and sort by coordinate.
2. **Mark Duplicates:** Flag PCR duplicates to avoid false positives.
3. **BQSR (Base Quality Score Recalibration):** Correct systematic errors in the sequencing machine's quality scores using known variants.

#### 3-1. Sort and Index

```
spack load samtools

for sid in $SAMPLES
do
  echo "Processing sample ${sid}..."
  
  # Convert to BAM (unsorted)
  /usr/bin/time -v samtools view -@ 32 -b ${sid}.sam -o ${sid}.unsorted.bam
  
  # Sort BAM
  /usr/bin/time -v samtools sort -@ 32 -o ${sid}.sorted.bam ${sid}.unsorted.bam
  
  # Index
  samtools index ${sid}.sorted.bam
  
  echo "Finished ${sid}: created sorted.bam and index"
done
```

#### 3-2. Mark Duplicates

```
spack load gatk

for sid in $SAMPLES
do
  echo "Marking duplicates for sample ${sid}..."

  gatk MarkDuplicates \
    -I ${sid}.sorted.bam \
    -O ${sid}.dedup.bam \
    -M ${sid}.markdup.metrics.txt \
    --TMP_DIR /scratch/username \

  samtools index ${sid}.dedup.bam
  
  echo "Finished ${sid}: deduplicated BAM created."
done
```

#### 3-3. BQSR (Base Recalibration)

```
# 1. Analyze patterns of covariation in the sequence data
for sid in $SAMPLES
do
  gatk BaseRecalibrator \
    -R ${REF} \
    -I ${sid}.dedup.bam \
    --known-sites ${REF_DIR}/dbsnp.vcf.gz \
    --known-sites ${REF_DIR}/Mills_indels.vcf.gz \
    -O ${sid}.recal_data.table
done

# 2. Apply the recalibration to the BAM file
for sid in $SAMPLES
do
  gatk ApplyBQSR \
    -R ${REF} \
    -I ${sid}.dedup.bam \
    --bqsr-recal-file ${sid}.recal_data.table \
    -O ${sid}.recal.bam

  # Index the final BAM
  samtools index ${sid}.recal.bam
done
```

### 4. Variant Calling (BAM â†’ VCF)

**Location:** `/<project_path>/<family_id>/vcf`

**Why?** Identify positions where the sample's genome differs from the reference. 

#### 4-1. HaplotypeCaller

```
spack load gatk

BAM_DIR=../bam
REF=/path/to/project/ref/Homo_sapiens_assembly38_plus.fasta

for sid in $SAMPLES
do
  echo "Running HaplotypeCaller for sample ${sid} ..."

  gatk HaplotypeCaller \
    -R ${REF} \
    -I ${BAM_DIR}/${sid}.recal.bam \
    -O ${sid}.vcf.gz \
    --native-pair-hmm-threads 32

  echo "Finished ${sid}: ${sid}.vcf.gz created"
done
```

#### 4-2. Normalization (Left-align & Split Multiallelics)

**Why?** Standardizes the representation of variants (especially Indels) and splits multiallelic sites (e.g., A -> T,G becomes two records) for easier downstream annotation.

```
spack load bcftools

for sid in $SAMPLES
do
  echo "â–¶ bcftools norm: split multiallelic + indel normalization for ${sid}..."

  bcftools norm \
    -m -both \
    -f "${REF}" \
    --threads 32 \
    ${sid}.vcf.gz \
    -Oz -o ${sid}.filtered.vcf.gz

  # Create index (.tbi)
  bcftools index -t --threads 16 ${sid}.filtered.vcf.gz

  echo "Done: ${sid}.filtered.vcf.gz ready for annotation."
done
```

### 5. Annotation (Overview)

**Note:** This step is currently performed locally, not on the HPC cluster. Detailed local instructions and Docker commands to be updated.

**Current Workflow:**
1. **Transfer:** Download the final `${sid}.filtered.vcf.gz` files to a local machine.
2. **Environment:** Launch a Docker container containing the annotation tools (VEP).
3. **Process:** Run the annotation pipeline against the GRCh38 database.
4. **Export:** Convert the annotated VCF to Excel format for clinical review.

